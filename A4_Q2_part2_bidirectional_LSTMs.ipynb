{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import pickle\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tensorflow.python.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n",
    "from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPAdamGaussianOptimizer\n",
    "from tensorflow.python.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, GRU, Concatenate, Dropout\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armandoordoricadelatorre/.pyenv/versions/3.7.3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (5,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('consumer_complaints.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(word_index):\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    with open(os.path.join(GLOVE_DIR, 'glove.6B.{0}d.txt'.format(EMBEDDING_DIM)), encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, num_words\n",
    "\n",
    "def data_preprocessing(texts, labels, indices, num_validation_samples, classifier=\"NeuralNetwork\"):\n",
    "    \n",
    "    word_index = None\n",
    "    \n",
    "    if classifier == \"NeuralNetwork\":\n",
    "        tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.hist([len(x) for x in sequences])\n",
    "        plt.xlabel('Length of Document')\n",
    "        plt.ylabel('Number of Documents')\n",
    "        plt.title('Statistic of Data')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        word_index = tokenizer.word_index\n",
    "        \n",
    "        with open('tokenizer.pickle', 'wb') as handle:\n",
    "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        \n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        data = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    labels = to_categorical(np.asarray(labels))  \n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "\n",
    "    x_train = data[:-num_validation_samples]\n",
    "    y_train = labels[:-num_validation_samples]\n",
    "    x_val = data[-num_validation_samples:]\n",
    "    y_val = labels[-num_validation_samples:]\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, word_index    \n",
    "\n",
    "def get_data(data_path):\n",
    "    texts = []  # list of text samples\n",
    "    labels_index = {}  # dictionary mapping label name to numeric id\n",
    "    labels = []  # list of label ids\n",
    "    for name in sorted(os.listdir(data_path)):\n",
    "        path = os.path.join(data_path, name)\n",
    "        if os.path.isdir(path):\n",
    "            label_id = len(labels_index)\n",
    "            labels_index[name] = label_id\n",
    "            for fname in sorted(os.listdir(path)):\n",
    "                if fname.isdigit():\n",
    "                    fpath = os.path.join(path, fname)\n",
    "                    args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                    with open(fpath, **args) as f:\n",
    "                        t = f.read()\n",
    "                        i = t.find('\\n\\n')  # Skip header which contains label!!!\n",
    "                        if 0 < i:\n",
    "                            t = t[i:]\n",
    "                        texts.append(t)\n",
    "                    labels.append(label_id)\n",
    "    return texts, labels_index, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_PATH = \"consumer_complaints.csv\"\n",
    "\n",
    "TEXT_DATA_DIR = consumer_PATH\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "import string\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import datetime\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning (partially modified)\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9()!?\\'\\`%$]\", \" \", string) # keep also %$ but removed comma\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\$\", \" $ \", string) #yes, isolate $\n",
    "    string = re.sub(r\"\\%\", \" % \", string) #yes, isolate %\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    # fixing XXX and xxx like as word\n",
    "    string = re.sub(r'\\S*(x{2,}|X{2,})\\S*',\"xxx\",string)\n",
    "    # removing non ascii\n",
    "    string = re.sub(r'[^\\x00-\\x7F]+', \"\", string) \n",
    "    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complaints(path):\n",
    "    \n",
    "    # Read the input dataset \n",
    "    d = pd.read_csv(path, \n",
    "                    usecols=('product','consumer_complaint_narrative'),\n",
    "                    dtype={'consumer_complaint_narrative': object})\n",
    "    # Only interested in data with consumer complaints\n",
    "    d=d[d['consumer_complaint_narrative'].notnull()]\n",
    "    d=d[d['product'].notnull()]\n",
    "    d.reset_index(drop=True,inplace=True)\n",
    "    d['consumer_complaint_narrative'] = d['consumer_complaint_narrative'].apply(clean_str)\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    label_name = d['product'].tolist()\n",
    "    le.fit(label_name)\n",
    "    d['label'] = le.transform(label_name)\n",
    "    return d['consumer_complaint_narrative'].tolist(), d['product'].tolist(), d['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels_index, labels = get_complaints(TEXT_DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
